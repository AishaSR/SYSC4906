{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYSC4906_Assig1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDOT_bqlnuaL"
      },
      "source": [
        "# SYSC4906 Introduction to Machine Learning\n",
        "## Assignment 1\n",
        "|**Student name** | **Student number**|\n",
        "|-----------------|-------------------|\n",
        "| TBC | TBC |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq842SciP6bK"
      },
      "source": [
        "# Question 1\n",
        "  i) Calculate the gradient of the following function: \n",
        "$$ f(x,y,z) \\stackrel{\\text{def}}{=} x^3z - 2xy^2 + 5z $$\n",
        "\n",
        "  ii) What does this vector represent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXV6T_R8QM4l"
      },
      "source": [
        "$$ \\Delta f(x,y,z)= \\begin{cases}{\\delta{TBC} \\over \\delta{TBC}} \\\\\n",
        "{\\delta{TBC} \\over \\delta{TBC}} \\\\\n",
        "{\\delta{TBC} \\over \\delta{TBC}}\\end{cases} $$\n",
        "\n",
        "$${\\delta{TBC} \\over \\delta{TBC}}= \\frac{TBC}{TBC}$$\n",
        "\n",
        "$$...$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayyHKf2kACcr"
      },
      "source": [
        "# Question 2\n",
        "*(see text of question in assignment instructions)*\n",
        "\n",
        "1.   What is the expected value from this sample? Using an unbiased estimator, what is the sample variance and standard deviation?\n",
        "\n",
        "2.   What is $Pr(2)$?\n",
        "\n",
        "3.   Find the expected value and the variance for $Pr(x)$.\n",
        "\n",
        "4.   Find the probability that a patient has a symptom severity of $x=5$, given that they tested negative for COVID-19. That is, find $Pr(5|-)$. Hint: $Pr(+)$ can be found by summing over $Pr(+|x)Pr(x)$, for all $x$. $Pr(-|x)$ can be derived from $Pr(+|x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F476OY19yL8C"
      },
      "source": [
        "#Question 3\n",
        "\n",
        "Create a python notebook which loads the Kaggle Heart attack possibility dataset (https://www.kaggle.com/nareshbhat/health-care-data-set-on-heart-attack-possibility). This dataset has 13 features each and 2 classes of heart attack possibility: target: 0= less chance of heart attack; 1= more chance of heart attack. *Hint: look at the notebooks from Tutorials 2 & 3 for example code for achieving the steps below.* \n",
        "\n",
        "    a) Split the data, using 75% for training and 25% for test. Make sure you use stratified sampling. \n",
        "\n",
        "    b) Train and test a logistic regression classifier. How accurate is your classifier?\n",
        "    c) Repeat part b), only the age and resting blood pressure features from the dataset. Was the classifier accuracy impacted?\n",
        "    d) Using the (two feature) classifier from part c), create two subplots using the first two features from the data set. \n",
        "    \n",
        "        i)  On the first, plot the decision boundary and the training data.  Use green for less chance (target==0) and blue for more chance (target==1).\n",
        "        ii) On the second, plot the decision boundary and the test data. Use the same colours (blue/green), but highlight all misclassified test points (from either class) in red.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1-XuzEdo4ts"
      },
      "source": [
        "# Load libraries..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCZnsSnbybUQ"
      },
      "source": [
        "## Q3.a) Create the dataset\n",
        "\n",
        "The first step is loading the Kaggle Heart Attack data. We will then split off the test data to be used for all training sets. Then create each training set, using **stratified sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3id7ezjlsn-J"
      },
      "source": [
        "# Load the Kaggle Heart Attack dataset\n",
        "kaggleData = \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5UPspGuGNga"
      },
      "source": [
        "# Question 4\n",
        "\n",
        "Linear regression. Download the file “Assig1Q3.csv” from GitHub under “Assignments/Assignment1”. The first column represents the X values, while the second column represents the Y values.\n",
        "* Plot the data\n",
        "\n",
        "We are going to use linear regression to fit a linear and a quadratic model to these data. Without using sklearn.linear_model (or any other linear regression libraries), write your own python code to implement the least squares solution for linear regression. That is:\n",
        "$$\\beta=(X^TX)^{−1}X^Ty$$\n",
        "\n",
        "* Assuming the model $y=mx+b$, use your code to best-fit the parameters $m$ and $b$ to the data. Report your optimal parameter values. \n",
        "*Hints: \n",
        "    * recall that you must create the ‘augmented’ feature vector $X$ from the given $x$ data (add a column of 1’s). \n",
        "    * look at numpy.T(), numpy.matmul(), numpy.dot(), and numpy.linalg.inv()\n",
        "* Plot your line of best fit on top of the data\n",
        "* Calculate the sum of square residuals, or mean squared error, as in:\n",
        "$$MSE(\\beta) = \\sum_{i=1}^{N}{(y−X\\beta)^2}$$\n",
        "* Assuming the model $y=ax^2+bx+c$, repeat steps 2-4 using this new model (i.e. estimate the optimal values for $a$,$b$,$c$; report those estimates; plot the line of best fit; report the MSE).\n",
        "* Briefly discuss which model would you prefer for these data?\n",
        "* Why is best-fitting the second (quadratic) model still considered linear regression?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EnFpwBiY5i4"
      },
      "source": [
        "## Step 1: Load the CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVGXLK2HZCQ6"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('Assig1Q3.csv',header=None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-EMZyfAb89b"
      },
      "source": [
        "## Step 2: Linear model $y=mx+b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVf8TwYMbx70"
      },
      "source": [
        "# Augment the x vector\n",
        "\n",
        "# Compute beta\n",
        "\n",
        "# Compute the MSE\n",
        "\n",
        "print(\"MSE =\", MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRN5Hhncge8y"
      },
      "source": [
        "## Step3: Quadratic model $y=ax^2+bx+c$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZWNuz5tge80"
      },
      "source": [
        "# Augment the x vector\n",
        "\n",
        "# Compute beta\n",
        "\n",
        "# Compute the MSE\n",
        "\n",
        "print(\"MSE =\", MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQMvagUYykXa"
      },
      "source": [
        "# Question 5\n",
        "\n",
        "- Create a Jupyter Notebook based on `Tutorial-3_ComparingMultipleClassifiers.ipynb` to use `make_classification` to create a linearly separable dataset, with 2 classes, 2 informative features, 1500 samples per class, using a class_sep=1.7, and a random_state of 5. \n",
        "- Generate some random noise of the same shape as your feature data, drawn from a standard normal distribution (see `numpy.random`) and a random_state of 5. \n",
        "- Create four datasets: \n",
        "    1. no noise, \n",
        "    2. data + 0.5 * noise, \n",
        "    3. data + 1.0 * noise, \n",
        "    4. data + 2.0 * noise. \n",
        "- i) For all four datasets, plot the data, labelling each (sub)plot by the degree of noise added (i.e. 0, 0.5, 1.0, and 2.0)\n",
        "- ii) For each dataset, create training and test data using a 70/30 train/test split (see train_test_split).\n",
        "- iii) For each dataset, train and test an SVM classifier with a polynomial kernel with `degree=2`, and `C=1.0`. Report the test score for each. How does prediction accuracy change with noise level?\n",
        "- iv) For a noise level of 0.5, train and test SVM classifiers using the following values for $C: \\{0.001, 0.01, 0.1, 1, 10, 100\\}$. \n",
        "   - Report the test accuracy for each. \n",
        "   - How does performance vary with $C$?\n",
        "   - Briefly describe what the $C$ controls for sklearn.svc. *Hint: look at the documentation for `sklearn.svc` rather than the class notes here...*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZYt5NmSvzm"
      },
      "source": [
        "# Load the required libraries...\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}